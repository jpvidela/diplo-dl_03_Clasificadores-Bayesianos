{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5MYaQwP0dLGf"
   },
   "source": [
    "# Naive Bayes Discreto\n",
    "\n",
    "Haremos un clasificador de artículos utilizando un modelo de Naive Bayes discreto. Trabajaremos con el dataset de Twenty News Group. Antes de empezar carguemos el dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cnSlIWEac1mm"
   },
   "outputs": [],
   "source": [
    "#Loading the data set - training data.\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "twenty_train = fetch_20newsgroups(subset='train', shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9m-XZO1Gc2v_"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_train.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OvtYC0qndcGJ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11314"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(twenty_train.data) #Cantidad de artículos periodísticos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R2MddId4d7EF"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(twenty_train[\"target\"]) #Clasificaciones de los artículos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fJZkpO1SeAcq"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_train[\"target_names\"] #Referencia de los números de target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X0byXL5-eFcx"
   },
   "outputs": [],
   "source": [
    "twenty_train.data[0] # Primer artículo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "alhF9Z41eQhA"
   },
   "source": [
    "Vamos a aplicar el siguiente procesamiento utilizando los conceptos vistos en clase:\n",
    "\n",
    "* Tokenization (nltk)\n",
    "* Lemmatization (nltk)\n",
    "* Stop Words (nltk)\n",
    "* Stemming (nltk)\n",
    "* Filtrado de palabras\n",
    "* Obtención del vocabulario (countvectorizer)\n",
    "* Transformación de los artículos en vectores\n",
    "* Armado del modelo de Naive Bayes Multinomial\n",
    "* Evaluación con el Train Set\n",
    "* Evaluación con el Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vVNxGZHMeJsH"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\jvidela\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jvidela\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jvidela\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RtB9WY00ewxM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "se procesaron 0 artículos\n",
      "se procesaron 1000 artículos\n",
      "se procesaron 2000 artículos\n",
      "se procesaron 3000 artículos\n",
      "se procesaron 4000 artículos\n",
      "se procesaron 5000 artículos\n",
      "se procesaron 6000 artículos\n",
      "se procesaron 7000 artículos\n",
      "se procesaron 8000 artículos\n",
      "se procesaron 9000 artículos\n",
      "se procesaron 10000 artículos\n",
      "se procesaron 11000 artículos\n"
     ]
    }
   ],
   "source": [
    "#Procesando todos los artículos:\n",
    "articulos_procesados=list()\n",
    "for idx in range(len(twenty_train.data)):\n",
    "    if idx%1000==0:\n",
    "        print(f'se procesaron {idx} artículos')\n",
    "    art=twenty_train.data[idx]\n",
    "    tok=word_tokenize(art)\n",
    "    lem=[lemmatizer.lemmatize(x, pos='v') for x in tok]\n",
    "    stop = [x for x in lem if x not in stopwords.words('english')]\n",
    "    stem=[stemmer.stem(x) for x in stop]\n",
    "    alpha=[x for x in stem if x.isalpha()]\n",
    "    articulos_procesados.append(\" \".join(alpha))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PKFI92vSkpv3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(max_df=0.8, min_df=20)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extracting features from articles\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer(max_df=0.8,min_df=20) #chequear que max_df y min_df sean los que se piden\n",
    "count_vect.fit(articulos_procesados) #Aprende el vocabulario y le asigna un código a cada palabra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bNuKEHS8nJgz"
   },
   "source": [
    "En la siguiente celda de código transforme los artículos procesados al vector de cuentas de palabras, es decir, transforme los artículos procesados utilizando el count vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n0aDvu7_niWR"
   },
   "outputs": [],
   "source": [
    "X_train_data="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xvp0yqMvn5u8"
   },
   "source": [
    "Utilice la función MultinomialNB de sklear para implementar un clasificador Naive Bayes discreto. Utilice smoothing laplaciano con alpha=3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SKbjTWPPlYU3"
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = #SOLUCIÓN AQUÍ\n",
    "clf.fit(#SOLUCION AQUI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QtG52ZJdpMOV"
   },
   "source": [
    "## Evaluación con el train set\n",
    "Evalúe el accuracy del modelo entrenado utilizando el train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aXk0xVJrnxk-"
   },
   "outputs": [],
   "source": [
    "clf.score(#SOLUCION AQUI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gYCjQykXq0of"
   },
   "source": [
    "# Evaluación con el test set\n",
    "Procese y convierta los artículos del test-set. Evalúe el accuracy del modelo con los parámetros obtenidos anteriormente utilizando el test-set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x_T_6JWGoedN"
   },
   "outputs": [],
   "source": [
    "twenty_test = fetch_20newsgroups(subset='test', shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BoX-jgDdpJlf"
   },
   "outputs": [],
   "source": [
    "#Procesando todos los artículos:\n",
    "articulos_procesados_test=list()\n",
    "for idx in range(len(twenty_test.data)):\n",
    "    ### SOLUCIÓN AQUÍ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WDJlssjrrlA7"
   },
   "outputs": [],
   "source": [
    "#Transforme los artículos de test procesados\n",
    "X_test_data="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "abq_GeMFpw8c"
   },
   "outputs": [],
   "source": [
    "#Evalúe el score del modelo entrenado para el train set para los artículos de test\n",
    "clf.score(#SOLUCION AQUI)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Laboratorio .ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python [conda env:diplo]",
   "language": "python",
   "name": "conda-env-diplo-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
